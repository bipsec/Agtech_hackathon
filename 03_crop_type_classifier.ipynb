{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c287202",
   "metadata": {},
   "source": [
    "### Imports + load chips + labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e01b1c",
   "metadata": {},
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PROC_DIR = \"data/processed\"\n",
    "RAW_DIR = \"data/raw\"\n",
    "DATE_STACK = [\"0626\", \"0710\", \"0731\"]   # best: use all three dates\n",
    "\n",
    "LABEL_CSV = os.path.join(RAW_DIR, \"labels\", \"crop_type.csv\")  # subplot_id,crop (5 classes)\n",
    "SPLIT_OUT = os.path.join(PROC_DIR, \"splits\")\n",
    "os.makedirs(SPLIT_OUT, exist_ok=True)\n",
    "\n",
    "labels = pd.read_csv(LABEL_CSV)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1b906",
   "metadata": {},
   "source": [
    "### Dataset: stack multiple dates into one tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922740e0",
   "metadata": {},
   "source": [
    "def load_npz(path):\n",
    "    z = np.load(path, allow_pickle=False)\n",
    "    x = z[\"x\"]\n",
    "    meta = json.loads(str(z[\"meta\"]))\n",
    "    return x, meta\n",
    "\n",
    "class MultiDateSubplotDataset(Dataset):\n",
    "    def __init__(self, ids, label_map, proc_dir, dates):\n",
    "        self.ids = ids\n",
    "        self.label_map = label_map\n",
    "        self.proc_dir = proc_dir\n",
    "        self.dates = dates\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.ids[idx]\n",
    "        xs = []\n",
    "        for d in self.dates:\n",
    "            p = os.path.join(self.proc_dir, \"subplots\", f\"chips_{d}\", f\"{sid}.npz\")\n",
    "            x, _ = load_npz(p)\n",
    "            xs.append(x)\n",
    "        x = np.concatenate(xs, axis=0)  # (C_total,H,W)\n",
    "        y = self.label_map[sid]\n",
    "        return torch.from_numpy(x).float(), torch.tensor(y).long(), sid\n",
    "\n",
    "label_map = {str(r.subplot_id): int(r.crop) for r in labels.itertuples()}\n",
    "all_ids = [sid for sid in label_map.keys() if all(os.path.exists(os.path.join(PROC_DIR, \"subplots\", f\"chips_{d}\", f\"{sid}.npz\")) for d in DATE_STACK)]\n",
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1725a2",
   "metadata": {},
   "source": [
    "### Split (grouped if you have plot_id; otherwise simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a78dc",
   "metadata": {},
   "source": [
    "# If you have plot_id in labels, group-split by plot_id. Otherwise random split.\n",
    "import random\n",
    "random.shuffle(all_ids)\n",
    "n = len(all_ids)\n",
    "train_ids = all_ids[:int(0.7*n)]\n",
    "val_ids   = all_ids[int(0.7*n):int(0.85*n)]\n",
    "test_ids  = all_ids[int(0.85*n):]\n",
    "\n",
    "with open(os.path.join(SPLIT_OUT,\"train_ids.json\"),\"w\") as f: json.dump(train_ids,f)\n",
    "with open(os.path.join(SPLIT_OUT,\"val_ids.json\"),\"w\") as f: json.dump(val_ids,f)\n",
    "with open(os.path.join(SPLIT_OUT,\"test_ids.json\"),\"w\") as f: json.dump(test_ids,f)\n",
    "len(train_ids), len(val_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a631c",
   "metadata": {},
   "source": [
    "### Simple classifier (custom first conv for C channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc86bf",
   "metadata": {},
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def make_resnet18(in_ch, n_classes=5):\n",
    "    m = models.resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    m.fc = nn.Linear(m.fc.in_features, n_classes)\n",
    "    return m\n",
    "\n",
    "C_total = None\n",
    "tmp_x,_,_ = MultiDateSubplotDataset([train_ids[0]], label_map, PROC_DIR, DATE_STACK)[0]\n",
    "C_total = tmp_x.shape[0]\n",
    "model = make_resnet18(C_total, n_classes=5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b3ba5",
   "metadata": {},
   "source": [
    "### Train loop + export corn_subplots.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3cf01",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "train_ds = MultiDateSubplotDataset(train_ids, label_map, PROC_DIR, DATE_STACK)\n",
    "val_ds   = MultiDateSubplotDataset(val_ids,   label_map, PROC_DIR, DATE_STACK)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "best_f1 = -1\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    for x,y,_ in train_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = crit(logits,y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y,_ in val_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(1).cpu().numpy().tolist()\n",
    "            ys += y.numpy().tolist()\n",
    "            ps += pred\n",
    "    f1 = f1_score(ys, ps, average=\"macro\")\n",
    "    print(\"epoch\", epoch, \"val macro-f1\", f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), os.path.join(PROC_DIR, \"crop_classifier_resnet18.pt\"))\n",
    "\n",
    "# Inference on all subplots -> select corn IDs (adjust class index/name for corn)\n",
    "CORN_CLASS = 0  # <-- set correctly based on your label encoding\n",
    "model.load_state_dict(torch.load(os.path.join(PROC_DIR, \"crop_classifier_resnet18.pt\"), map_location=device))\n",
    "model.eval()\n",
    "\n",
    "corn_ids = []\n",
    "with torch.no_grad():\n",
    "    for sid in all_ids:\n",
    "        x,_,_ = MultiDateSubplotDataset([sid], label_map, PROC_DIR, DATE_STACK)[0]\n",
    "        logits = model(x[None].to(device))\n",
    "        pred = int(logits.argmax(1).item())\n",
    "        if pred == CORN_CLASS:\n",
    "            corn_ids.append(sid)\n",
    "\n",
    "corn_path = os.path.join(PROC_DIR, \"corn_subplots.json\")\n",
    "with open(corn_path,\"w\") as f: json.dump(corn_ids,f)\n",
    "corn_path, len(corn_ids)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
